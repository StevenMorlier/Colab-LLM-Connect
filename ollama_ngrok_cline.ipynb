{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenMorlier/Colab-LLM-Connect/blob/main/ollama_ngrok_cline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows how to install and run ollama on google colab + tunneling using ngrok.\n",
        "\n",
        "set `NGROK_TOKEN` in colab secrets panel.\n",
        "\n",
        "Gholamreza Dar 2024"
      ],
      "metadata": {
        "id": "x_PlD2xgGkyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab"
      ],
      "metadata": {
        "id": "3KvecN0uFnHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "template pour cline :\n",
        "https://github.com/maryasov/ollama-models-instruct-for-cline.git"
      ],
      "metadata": {
        "id": "swLHkXCVBOsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "definir le modele choisi"
      ],
      "metadata": {
        "id": "jMGwGBDoID4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# D√©finir le mod√®le de base ICI\n",
        "#MODEL_NAME = \"qwen2.5-coder:7b\" # <---\n",
        "MODEL_NAME = \"llama3.1:8b\" # <---\n",
        "MODEL_NAME = \"deepseek-r1:14b\" # <---\n",
        "\n",
        "\n",
        "# Cr√©er automatiquement le nom du mod√®le avec template en rempla√ßant ce qui est apr√®s les \":\"\n",
        "MODEL_WITH_TEMPLATE = re.sub(r'(.*:).*', r'\\1cline', MODEL_NAME)\n",
        "\n",
        "print(f\"Mod√®le de base: {MODEL_NAME}\")\n",
        "print(f\"Mod√®le avec template: {MODEL_WITH_TEMPLATE}\")"
      ],
      "metadata": {
        "id": "EnYfklkoICza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test GPU"
      ],
      "metadata": {
        "id": "7MWM-eV8tvTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y lshw\n",
        "!apt-get install -y pciutils\n",
        "!nvcc --version\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "  from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('‚ùå Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('‚úÖ You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "nqsL_7hdtuNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ldl_BkTYOBAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd4T8NagB0Rd"
      },
      "outputs": [],
      "source": [
        "# Installing Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# needed for tunneling via ngrok\n",
        "%env OLLAMA_HOST=0.0.0.0\n",
        "# Running Ollama and serving on localhost:11434\n",
        "!nohup ollama serve &\n",
        "!ollama pull $MODEL_NAME"
      ],
      "metadata": {
        "id": "ApJQZESPOkUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Variables globales pour les noms des mod√®les\n",
        "#MODEL_NAME = \"qwen2.5-coder:32b\"  # Mod√®le de base\n",
        "#MODEL_WITH_TEMPLATE = re.sub(r'(.*:).*', r'\\1cline', MODEL_NAME)  # Mod√®le avec template\n",
        "\n",
        "def create_cline_modelfile(\n",
        "    base_model=MODEL_NAME,\n",
        "    model_name=MODEL_WITH_TEMPLATE,\n",
        "    modelfile_path=None,\n",
        "    create_model=True,\n",
        "    parameters={\n",
        "        \"num_ctx\": 32768,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.7,\n",
        "        \"presence_penalty\": 1.0,\n",
        "        \"frequency_penalty\": 1.0,\n",
        "        \"stop\": [\"<|im_start|>\", \"<|im_end|>\"]  # Ajout des deux valeurs d'arr√™t\n",
        "    }\n",
        "):\n",
        "    \"\"\"\n",
        "    Cr√©e un Modelfile pour Ollama avec le template Cline et cr√©e optionnellement le mod√®le.\n",
        "    Utilise les variables globales MODEL_NAME et MODEL_WITH_TEMPLATE par d√©faut.\n",
        "\n",
        "    Args:\n",
        "        base_model: Mod√®le de base √† utiliser (par d√©faut: MODEL_NAME)\n",
        "        model_name: Nom du mod√®le √† cr√©er (par d√©faut: MODEL_WITH_TEMPLATE)\n",
        "        modelfile_path: Chemin o√π cr√©er le Modelfile (si None, d√©riv√© du nom du mod√®le)\n",
        "        create_model: Si True, ex√©cute la commande ollama create\n",
        "        parameters: Dictionnaire des param√®tres √† utiliser pour le mod√®le\n",
        "\n",
        "    Returns:\n",
        "        str: Chemin du Modelfile cr√©√©\n",
        "    \"\"\"\n",
        "    # Si modelfile_path n'est pas sp√©cifi√©, le d√©river du nom du mod√®le\n",
        "    if modelfile_path is None:\n",
        "        model_short_name = model_name.split(':')[0]\n",
        "        modelfile_path = f\"Modelfile.{model_short_name}\"\n",
        "\n",
        "    # Template Cline avanc√© avec support des outils\n",
        "    template_content = \"\"\"{{- /* Initial system message with core instructions */ -}}\n",
        "{{- if .Messages }}\n",
        "{{- if or .System .Tools }}\n",
        "<|im_start|>system\n",
        "{{- if .System }}\n",
        "{{ .System }}\n",
        "{{- end }}\n",
        "\n",
        "{{- if .Tools }}\n",
        "# Tools and XML Schema\n",
        "You have access to the following tools. Each tool must be used according to this XML schema:\n",
        "\n",
        "<tools>\n",
        "{{- range .Tools }}\n",
        "{{ .Function }}\n",
        "{{- end }}\n",
        "</tools>\n",
        "\n",
        "## Tool Use Format\n",
        "1. Think about the approach in <thinking> tags\n",
        "2. Call tool using XML format:\n",
        "   <tool_name>\n",
        "     <param_name>value</param_name>\n",
        "   </tool_name>\n",
        "3. Process tool response from:\n",
        "   <tool_response>result</tool_response>\n",
        "{{- end }}\n",
        "<|im_end|>\n",
        "{{- end }}\n",
        "\n",
        "{{- /* Message handling loop */ -}}\n",
        "{{- range $i, $_ := .Messages }}\n",
        "{{- $last := eq (len (slice $.Messages $i)) 1 }}\n",
        "\n",
        "{{- /* User messages */ -}}\n",
        "{{- if eq .Role \"user\" }}\n",
        "<|im_start|>user\n",
        "{{ .Content }}\n",
        "<|im_end|>\n",
        "\n",
        "{{- /* Assistant messages */ -}}\n",
        "{{- else if eq .Role \"assistant\" }}\n",
        "<|im_start|>assistant\n",
        "{{- if .Content }}\n",
        "{{ .Content }}\n",
        "{{- else if .ToolCalls }}\n",
        "{{- range .ToolCalls }}\n",
        "<thinking>\n",
        "[Analysis of current state and next steps]\n",
        "</thinking>\n",
        "\n",
        "<{{ .Function.Name }}>\n",
        "{{- range $key, $value := .Function.Arguments }}\n",
        "<{{ $key }}>{{ $value }}</{{ $key }}>\n",
        "{{- end }}\n",
        "</{{ .Function.Name }}>\n",
        "{{- end }}\n",
        "{{- end }}\n",
        "<|im_end|>\n",
        "\n",
        "{{- /* Tool response handling */ -}}\n",
        "{{- else if eq .Role \"tool\" }}\n",
        "<|im_start|>tool\n",
        "<tool_response>\n",
        "{{ .Content }}\n",
        "</tool_response>\n",
        "<|im_end|>\n",
        "{{- end }}\n",
        "\n",
        "{{- /* Prepare for next assistant response if needed */ -}}\n",
        "{{- if and (ne .Role \"assistant\") $last }}\n",
        "<|im_start|>assistant\n",
        "{{- end }}\n",
        "{{- end }}\n",
        "\n",
        "{{- /* Handle single message case */ -}}\n",
        "{{- else }}\n",
        "{{- if .System }}\n",
        "<|im_start|>system\n",
        "{{ .System }}\n",
        "<|im_end|>\n",
        "{{- end }}\n",
        "\n",
        "{{- if .Prompt }}\n",
        "<|im_start|>user\n",
        "{{ .Prompt }}\n",
        "<|im_end|>\n",
        "{{- end }}\n",
        "<|im_start|>assistant\n",
        "{{- end }}\n",
        "{{ .Response }}<|im_end|>\"\"\"\n",
        "\n",
        "    # Cr√©er le Modelfile\n",
        "    with open(modelfile_path, 'w') as f:\n",
        "        # La ligne FROM est obligatoire\n",
        "        f.write(f'FROM {base_model}\\n\\n')\n",
        "\n",
        "        # Commencer la d√©finition du template\n",
        "        f.write('TEMPLATE \"\"\"\\n')\n",
        "        f.write(template_content + '\\n\"\"\"\\n\\n')\n",
        "\n",
        "        # Ajouter les param√®tres\n",
        "        for param_name, param_value in parameters.items():\n",
        "            # G√©rer les cas sp√©ciaux qui n√©cessitent des guillemets\n",
        "            if param_name == \"stop\":\n",
        "                if isinstance(param_value, list):\n",
        "                    for stop_value in param_value:\n",
        "                        f.write(f'PARAMETER stop \"{stop_value}\"\\n')\n",
        "                else:\n",
        "                    f.write(f'PARAMETER stop \"{param_value}\"\\n')\n",
        "            else:\n",
        "                f.write(f'PARAMETER {param_name} {param_value}\\n')\n",
        "\n",
        "    print(f\"‚úÖ Modelfile cr√©√©: {modelfile_path}\")\n",
        "    print(f\"   Base model: {base_model}\")\n",
        "    print(f\"   Template model: {model_name}\")\n",
        "\n",
        "    # Afficher le contenu du Modelfile (optionnel)\n",
        "    # print(\"\\n--- Contenu du Modelfile ---\")\n",
        "    # with open(modelfile_path, 'r') as f:\n",
        "    #     print(f.read())\n",
        "    # print(\"---------------------------\\n\")\n",
        "\n",
        "    # Cr√©er le mod√®le si demand√©\n",
        "    if create_model:\n",
        "        print(f\"üîÑ Cr√©ation du mod√®le {model_name}...\")\n",
        "        cmd = f\"ollama create {model_name} -f {modelfile_path}\"\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"‚úÖ Mod√®le {model_name} cr√©√© avec succ√®s!\")\n",
        "        else:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation du mod√®le: {result.stderr}\")\n",
        "\n",
        "    return modelfile_path\n",
        "\n",
        "# Fonction pour utiliser le mod√®le (avec ou sans template)\n",
        "def generate_with_model(prompt, use_template=True, temperature=0.1):\n",
        "    \"\"\"\n",
        "    G√©n√®re du texte en utilisant soit le mod√®le de base, soit le mod√®le avec template\n",
        "\n",
        "    Args:\n",
        "        prompt: Le prompt √† envoyer au mod√®le\n",
        "        use_template: Si True, utilise MODEL_WITH_TEMPLATE, sinon MODEL_NAME\n",
        "        temperature: Temp√©rature pour la g√©n√©ration\n",
        "\n",
        "    Returns:\n",
        "        str: La r√©ponse du mod√®le\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    model = MODEL_WITH_TEMPLATE if use_template else MODEL_NAME\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "\n",
        "    cmd = f'''\n",
        "    curl http://localhost:11434/api/generate -d '{json.dumps(data)}' -s\n",
        "    '''\n",
        "\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    return result.stdout\n",
        "\n",
        "# Exemple d'utilisation\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Mod√®le de base: {MODEL_NAME}\")\n",
        "    print(f\"Mod√®le avec template: {MODEL_WITH_TEMPLATE}\")\n",
        "\n",
        "    # Cr√©er le mod√®le avec template\n",
        "    create_cline_modelfile()\n",
        "\n",
        "    # Exemple d'utilisation des mod√®les\n",
        "    #print(\"\\nTest du mod√®le de base:\")\n",
        "    #response = generate_with_model(\"√âcris une fonction Python qui calcule la factorielle\", use_template=False)\n",
        "    #print(response)\n",
        "\n",
        "    #print(\"\\nTest du mod√®le avec template:\")\n",
        "    #response = generate_with_model(\"√âcris une fonction Python qui calcule la factorielle\", use_template=True)\n",
        "    #print(response)\n",
        "    # Check the available models\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "9D7-cYb-Kosk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curl"
      ],
      "metadata": {
        "id": "3hTq9RR0ElgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the API using curl locally with the MODEL_WITH_TEMPLATE variable\n",
        "import json\n",
        "\n",
        "# Pr√©paration des donn√©es avec la variable MODEL_WITH_TEMPLATE\n",
        "data = {\n",
        "    \"model\": MODEL_WITH_TEMPLATE,\n",
        "    \"prompt\": \"Question: Who was the first president of the United States? \\n Only answer using a few words. maybe just a name Answer: \"\n",
        "}\n",
        "\n",
        "# Construction de la commande curl avec les donn√©es JSON\n",
        "!curl http://localhost:11434/api/generate -d '{json.dumps(data)}'"
      ],
      "metadata": {
        "id": "zl1ulZMmGf-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expose the API publicly using ngrok"
      ],
      "metadata": {
        "id": "QVNlbt4THf3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pyngrok"
      ],
      "metadata": {
        "id": "6RSy-7MZHwoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# get NGROK_TOKEN from colab secrets\n",
        "ngrok_token = userdata.get('NGROK_TOKEN')\n",
        "if not ngrok_token:\n",
        "    raise ValueError(\"NGROK_TOKEN secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "# Set the ngrok auth token using Python\n",
        "conf.get_default().auth_token = ngrok_token\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Expose Ollama server via ngrok on port 11434\n",
        "public_url = ngrok.connect(\"http://localhost:11434\")\n",
        "print(f\"Ollama server public URL for CLINE: {public_url.public_url}\")\n",
        "print(f\"Model: {MODEL_WITH_TEMPLATE} (do NOT add / at the end in cline url)\")\n",
        "print(f\"commande pour utiliser sur ollama en local\")\n",
        "print(f\"export OLLAMA_HOST={public_url.public_url}/\")"
      ],
      "metadata": {
        "id": "j9aXTx7oIFYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V89Af65WgcI2"
      },
      "cell_type": "code",
      "source": [
        "# Sleep for a few seconds.\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}