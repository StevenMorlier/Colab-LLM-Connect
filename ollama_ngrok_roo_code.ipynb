{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenMorlier/Colab-LLM-Connect/blob/main/ollama_ngrok_roo_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running Ollama on colab to test LLMs on Roo-Code Extention for VScode\n",
        "\n",
        "\n",
        "## THREE-STEP PROCESS\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "### 01 â”‚ SET UP NGROK\n",
        "\n",
        "â–¢ Add `NGROK_TOKEN` to Colab secrets panel (â—†)\n",
        "  \n",
        "  â–¹ Obtain token: ngrok.com (1 free tunnel available)\n",
        "\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "### 02 â”‚ EXECUTE NOTEBOOK\n",
        "\n",
        "â–¢ Select \"Runtime\" â†’ \"Run all\"\n",
        "\n",
        "  â–¹ Wait until completion signal\n",
        "\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "### 03 â”‚ CONNECT TO CLINE\n",
        "\n",
        "â–¢ Copy the generated ngrok URL\n",
        "  \n",
        "â–¢ Configure Roo-code settings with URL in the ollama API (remove the last \"/\")\n",
        "\n",
        "  â–¹ you're good to go ğŸš€\n",
        "\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "\n",
        "â”‚ MODEL CONFIGURATION                         â”‚\n",
        "\n",
        "Modify first cell to change model:\n",
        "\n",
        "MODEL_NAME = \"llama3.3:latest\"  # always specify :latest or size of the model (:8b or :14b...)\n"
      ],
      "metadata": {
        "id": "x_PlD2xgGkyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to Matthieu DELARO for is good advices,\n",
        "Gholamreza Dar & maryasov for for their works (links just below)\n",
        "\n",
        "https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
        "\n",
        "https://github.com/maryasov/ollama-models-instruct-for-cline.git"
      ],
      "metadata": {
        "id": "3KvecN0uFnHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zzf7d8NzwhGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#START HERE\n",
        "but if you want to change the model, do it below"
      ],
      "metadata": {
        "id": "jMGwGBDoID4x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKFLp9aFGZ7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"maryasov/qwen2.5-coder-cline:7b-instruct-q8_0\" # <--- Works well with Roo-code and T4 high RAM\n",
        "#MODEL_NAME = \"qwq:32b\" # <-- Slow, should find an optimised version for Roo...\n",
        "#MODEL_NAME = \"tom_himanen/deepseek-r1-roo-cline-tools:32b\" # <--- This one is too slow on T4\n",
        "#see differents models on https://github.com/maryasov/ollama-models-instruct-for-cline.git"
      ],
      "metadata": {
        "id": "KDkWtD0OsKBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## colab GPU basic setting for Ollama"
      ],
      "metadata": {
        "id": "7MWM-eV8tvTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y lshw\n",
        "!apt-get install -y pciutils\n",
        "!nvcc --version\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "  from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('âŒ Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('âœ… You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "nqsL_7hdtuNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd4T8NagB0Rd"
      },
      "outputs": [],
      "source": [
        "# Installing Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# needed for tunneling via ngrok\n",
        "%env OLLAMA_HOST=0.0.0.0\n",
        "\n",
        "#Setting the size of the context\n",
        "#%env OLLAMA_CONTEXT_LENGTH=32768\n",
        "\n",
        "# Running Ollama and serving on localhost:11434\n",
        "!nohup ollama serve &\n",
        "!ollama pull {MODEL_NAME}"
      ],
      "metadata": {
        "id": "ApJQZESPOkUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test the model"
      ],
      "metadata": {
        "id": "3hTq9RR0ElgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the API using curl locally with the MODEL_WITH_TEMPLATE variable\n",
        "import json\n",
        "\n",
        "# PrÃ©paration des donnÃ©es avec la variable MODEL_WITH_TEMPLATE\n",
        "data = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"prompt\": \"Question: Who was the first president of the United States? \\n Only answer using a few words. maybe just a name Answer: \"\n",
        "}\n",
        "\n",
        "# Construction de la commande curl avec les donnÃ©es JSON\n",
        "!curl http://localhost:11434/api/generate -d '{json.dumps(data)}'"
      ],
      "metadata": {
        "id": "Iz4fNRKMHrOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expose the API publicly using ngrok"
      ],
      "metadata": {
        "id": "QVNlbt4THf3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pyngrok"
      ],
      "metadata": {
        "id": "6RSy-7MZHwoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# get NGROK_TOKEN from colab secrets\n",
        "ngrok_token = userdata.get('NGROK_TOKEN')\n",
        "if not ngrok_token:\n",
        "    raise ValueError(\"NGROK_TOKEN secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "# Set the ngrok auth token using Python\n",
        "conf.get_default().auth_token = ngrok_token\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Expose Ollama server via ngrok on port 11434\n",
        "public_url = ngrok.connect(\"http://localhost:11434\")\n",
        "print(f\"Ollama server public URL for CLINE: {public_url.public_url}\")\n",
        "print(f\"Model: {MODEL_NAME} (do NOT add / at the end in cline url)\")\n",
        "print(f\"commande pour utiliser sur ollama en local\")\n",
        "print(f\"export OLLAMA_HOST={public_url.public_url}/\")"
      ],
      "metadata": {
        "id": "j9aXTx7oIFYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V89Af65WgcI2"
      },
      "cell_type": "code",
      "source": [
        "# BEEP, work done !\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}