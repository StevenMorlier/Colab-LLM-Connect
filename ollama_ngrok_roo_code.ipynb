{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenMorlier/Colab-LLM-Connect/blob/main/ollama_ngrok_roo_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running Ollama on colab to test LLMs on Roo-Code Extention for VScode\n",
        "\n",
        "\n",
        "## THREE-STEP PROCESS\n",
        "━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "### 01 │ SET UP NGROK\n",
        "\n",
        "▢ Add `NGROK_TOKEN` to Colab secrets panel (◆)\n",
        "  \n",
        "  ▹ Obtain token: ngrok.com (1 free tunnel available)\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "### 02 │ EXECUTE NOTEBOOK\n",
        "\n",
        "▢ Select \"Runtime\" → \"Run all\"\n",
        "\n",
        "  ▹ Wait until completion signal\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "### 03 │ CONNECT TO CLINE\n",
        "\n",
        "▢ Copy the generated ngrok URL\n",
        "  \n",
        "▢ Configure Roo-code settings with URL in the ollama API (remove the last \"/\")\n",
        "\n",
        "  ▹ you're good to go 🚀\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "\n",
        "│ MODEL CONFIGURATION                         │\n",
        "\n",
        "Modify first cell to change model:\n",
        "\n",
        "MODEL_NAME = \"llama3.3:latest\"  # always specify :latest or size of the model (:8b or :14b...)\n"
      ],
      "metadata": {
        "id": "x_PlD2xgGkyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to Matthieu DELARO for is good advices,\n",
        "Gholamreza Dar & maryasov for for their works (links just below)\n",
        "\n",
        "https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
        "\n",
        "https://github.com/maryasov/ollama-models-instruct-for-cline.git"
      ],
      "metadata": {
        "id": "3KvecN0uFnHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zzf7d8NzwhGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#START HERE\n",
        "but if you want to change the model, do it below"
      ],
      "metadata": {
        "id": "jMGwGBDoID4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"maryasov/qwen2.5-coder-cline:7b-instruct-q8_0\" # <--- Works well with Roo-code\n",
        "MODEL_NAME = \"tom_himanen/deepseek-r1-roo-cline-tools:32b\"\n",
        "#see differents models on https://github.com/maryasov/ollama-models-instruct-for-cline.git"
      ],
      "metadata": {
        "id": "KDkWtD0OsKBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## colab GPU basic setting for Ollama"
      ],
      "metadata": {
        "id": "7MWM-eV8tvTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y lshw\n",
        "!apt-get install -y pciutils\n",
        "!nvcc --version\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "  from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('❌ Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('✅ You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "nqsL_7hdtuNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd4T8NagB0Rd"
      },
      "outputs": [],
      "source": [
        "# Installing Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# needed for tunneling via ngrok\n",
        "%env OLLAMA_HOST=0.0.0.0\n",
        "# Running Ollama and serving on localhost:11434\n",
        "!nohup ollama serve &\n",
        "!ollama pull {MODEL_NAME}"
      ],
      "metadata": {
        "id": "ApJQZESPOkUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test the model"
      ],
      "metadata": {
        "id": "3hTq9RR0ElgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Fonction simplifiée pour extraire juste la phrase et le temps\n",
        "def extraire_phrase(json_output):\n",
        "    # Si l'entrée est une chaîne de texte, la diviser en lignes\n",
        "    if isinstance(json_output, str):\n",
        "        json_lines = json_output.strip().split('\\n')\n",
        "    else:\n",
        "        json_lines = json_output\n",
        "\n",
        "    # Variables pour l'extraction\n",
        "    phrase_complete = \"\"\n",
        "    premier_timestamp = None\n",
        "    dernier_timestamp = None\n",
        "\n",
        "    # Parcourir les lignes JSON\n",
        "    for ligne in json_lines:\n",
        "        if not ligne.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            data = json.loads(ligne)\n",
        "\n",
        "            # Ajouter le token à la phrase\n",
        "            if 'response' in data:\n",
        "                phrase_complete += data['response']\n",
        "\n",
        "            # Extraire les timestamps\n",
        "            if 'created_at' in data:\n",
        "                timestamp = datetime.fromisoformat(data['created_at'].replace('Z', '+00:00'))\n",
        "                if premier_timestamp is None:\n",
        "                    premier_timestamp = timestamp\n",
        "                dernier_timestamp = timestamp\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Calculer le temps écoulé si disponible\n",
        "    temps_ecoule = None\n",
        "    if premier_timestamp and dernier_timestamp:\n",
        "        temps_ecoule = (dernier_timestamp - premier_timestamp).total_seconds()\n",
        "\n",
        "    return phrase_complete, temps_ecoule\n",
        "\n",
        "# Intégration avec votre commande curl existante\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Préparation des données\n",
        "data = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"prompt\": \"reverse the next sentence words by words including commas, and answer without commenting. Sentence is : I do so ,generation next the treck star watching love they\"\n",
        "}\n",
        "\n",
        "# Exécution de curl et capture de la sortie\n",
        "debut = time.time()\n",
        "resultat = !curl -s http://localhost:11434/api/generate -d '{json.dumps(data)}'\n",
        "fin = time.time()\n",
        "\n",
        "# Extraction de la phrase\n",
        "phrase, temps_api = extraire_phrase(resultat)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"\\nRéponse:\")\n",
        "print(phrase)\n",
        "\n",
        "if temps_api:\n",
        "    print(f\"\\nTemps de génération (selon l'API): {temps_api:.2f} secondes\")\n",
        "else:\n",
        "    print(f\"\\nTemps total (mesure locale): {fin - debut:.2f} secondes\")"
      ],
      "metadata": {
        "id": "Iz4fNRKMHrOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expose the API publicly using ngrok"
      ],
      "metadata": {
        "id": "QVNlbt4THf3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pyngrok"
      ],
      "metadata": {
        "id": "6RSy-7MZHwoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# get NGROK_TOKEN from colab secrets\n",
        "ngrok_token = userdata.get('NGROK_TOKEN')\n",
        "if not ngrok_token:\n",
        "    raise ValueError(\"NGROK_TOKEN secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "# Set the ngrok auth token using Python\n",
        "conf.get_default().auth_token = ngrok_token\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Expose Ollama server via ngrok on port 11434\n",
        "public_url = ngrok.connect(\"http://localhost:11434\")\n",
        "print(f\"Ollama server public URL for CLINE: {public_url.public_url}\")\n",
        "print(f\"Model: {MODEL_NAME} (do NOT add / at the end in cline url)\")\n",
        "print(f\"commande pour utiliser sur ollama en local\")\n",
        "print(f\"export OLLAMA_HOST={public_url.public_url}/\")"
      ],
      "metadata": {
        "id": "j9aXTx7oIFYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V89Af65WgcI2"
      },
      "cell_type": "code",
      "source": [
        "# BEEP, work done !\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}